{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchviz import make_dot\n",
    "\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from torch.jit.annotations import TensorType\n",
    "\n",
    "from IPython.display import Markdown as md # For automated updates of the table\n",
    "\n",
    "# import pickle # tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset and dataloaders\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = torchvision.datasets.MNIST(root=os.getcwd(), train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, val, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=os.cpu_count())\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=os.cpu_count())\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Below is a dictionary of the models used for the experiment, with different numbers of convolutional layers and fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter dictionary for each model and \n",
    "# corresponding layer parameters.\n",
    "params: Dict[str,Dict[str,torch.nn]] = {\n",
    "    \"model.1\": {\n",
    "        \"conv_layers\": (nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        ),\n",
    "        \"fc_layer\": (nn.Linear(256, 128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128, 10),)\n",
    "    },\n",
    "    \"model.2\": {\n",
    "        \"conv_layers\": (nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                        ),\n",
    "        \"fc_layer\": (nn.Linear(32, 32),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(32, 10),)\n",
    "    },\n",
    "    \"model.3\": {\n",
    "        \"conv_layers\": (nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                        ),\n",
    "        \"fc_layer\": (nn.Linear(128 * 3 * 3, 256),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(256, 10))\n",
    "    },\n",
    "    \"model.4\": {\n",
    "        \"conv_layers\": (nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                        ),\n",
    "        \"fc_layer\": (nn.Linear(64 * 7 * 7, 128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128, 10)\n",
    "                     )\n",
    "    },\n",
    "    \"model.5\": {\n",
    "        \"conv_layers\": (nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        ),\n",
    "        \"fc_layer\": (nn.Linear(32 * 14 * 14, 128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128, 10)\n",
    "                     )\n",
    "    },\n",
    "    \"model.6\": {\n",
    "        \"conv_layers\": (nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        ),\n",
    "        \"fc_layer\": (nn.Linear(14 * 14 * 32, 10))\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model: pl.LightningModule) -> int:\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_and_viz_pl_model(model: Union[nn.Module,pl.LightningModule], filename: str) -> None:\n",
    "    \"\"\"Helper function to visualize and plot the model architecture.\n",
    "\n",
    "    Args:\n",
    "        model: Input pytorch (lightning) model.\n",
    "        filename: Output filename (no file extension).\n",
    "    \"\"\"\n",
    "    # Create a dummy input with the same shape as expected input during training\n",
    "    dummy_input = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "    # Generate the visualization of the model architecture\n",
    "    dot = make_dot(model(dummy_input))\n",
    "    # params = dict(model.named_parameters())\n",
    "\n",
    "    # Save the visualization as an image\n",
    "    dot.format = 'png'\n",
    "    dot.render(filename, cleanup=True)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pt_model(model: Union[nn.Module,pl.LightningModule], filename: str) -> None:\n",
    "    \"\"\"Saves pytorch (lightning) model, and creates visualization of model architecture.\n",
    "\n",
    "    Args:\n",
    "        model: Input pytorch (lightning) model.\n",
    "        filename: Output filename.\n",
    "    \"\"\"\n",
    "    # TODO: Save metadata file for the model.\n",
    "    # Check filename\n",
    "    filename: str\n",
    "    ext: str\n",
    "    \n",
    "    if ('pt' or 'pth') in filename:\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "    else:\n",
    "        ext: str = \".pt\"\n",
    "    \n",
    "    # Save model (and model state)\n",
    "    torch.save(model.state_dict(), f\"{filename}{ext}\")\n",
    "    \n",
    "    # # pickle\n",
    "    # with open(f\"{filename}{ext}\",'wb') as f:\n",
    "    #     pickle.dump(model,f)\n",
    "\n",
    "    # Save image of model architecture\n",
    "    _save_and_viz_pl_model(model=model, filename=filename)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pt_model(model: Union[nn.Module,pl.LightningModule], filename: str) -> Union[nn.Module,pl.LightningModule]:\n",
    "    \"\"\"Loads saved/trained model, in which the model class **must** be provided.\n",
    "\n",
    "    Args:\n",
    "        model: Input model class objoect.\n",
    "        filename: Input filename that corresponds to trained saved/trained model.\n",
    "\n",
    "    Returns:\n",
    "        Trained model.\n",
    "    \"\"\"\n",
    "    if ('pt' or 'pth') in filename:\n",
    "        pass\n",
    "    else:\n",
    "        filename: str = f\"{filename}.pt\"\n",
    "\n",
    "    # # Pickle\n",
    "    # with open(filename,'rb') as f:\n",
    "    #     pickle.load(f)\n",
    "\n",
    "    # Load model\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    model.eval() # sets dropout and batch normalization layers to evaluation mode\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_norms(model: Union[nn.Module,pl.LightningModule], weight: bool = True, bias: bool = False) -> List[Tuple[str,float]]:\n",
    "    model.eval()\n",
    "    sample_input = torch.randn(1, 1, 28, 28)  # Replace with your own sample input\n",
    "    outputs = model(sample_input)\n",
    "    loss = torch.sum(outputs)  # Create a dummy loss\n",
    "\n",
    "    # Backpropagate to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Compute gradient norms\n",
    "    gradient_norms: List[Tuple[str,float]] = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if weight and ('weight' in name):\n",
    "                gradient_norms.append((name, param.grad.norm().item()))\n",
    "            \n",
    "            if bias and ('bias' in name):\n",
    "                gradient_norms.append((name, param.grad.norm().item()))\n",
    "                \n",
    "    return gradient_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layerwise_norms(model: Union[nn.Module,pl.LightningModule], weight: bool = True, bias: bool = False) -> List[Tuple[str,float]]:\n",
    "    model.eval()\n",
    "    sample_input = torch.randn(1, 1, 28, 28)  # Replace with your own sample input\n",
    "    outputs = model(sample_input)\n",
    "    loss = torch.sum(outputs)  # Create a dummy loss\n",
    "\n",
    "    # Backpropagate to compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    layerwise_norms: List[Tuple[str,float]] = []\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_name = name #.split('.')[0]  # Extract the layer name\n",
    "        norm = param.norm().item()\n",
    "\n",
    "        if weight and ('weight' in name):\n",
    "            layerwise_norms.append((layer_name, norm))\n",
    "        \n",
    "        if bias and ('bias' in name):\n",
    "            layerwise_norms.append((layer_name, norm))\n",
    "            \n",
    "    return layerwise_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_parameter_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        total_norm += param.norm().item()\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameter_norms_per_layer(model, weight: bool = True, bias: bool = False):\n",
    "    norms_per_layer = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_name = name #.split('.')[0]  # Extract the layer name\n",
    "        norm = param.norm().item()\n",
    "\n",
    "        if weight and ('weight' in name):\n",
    "            if layer_name not in norms_per_layer:\n",
    "                norms_per_layer[layer_name] = []\n",
    "            norms_per_layer[layer_name].append(norm)\n",
    "        \n",
    "        if bias and ('bias' in name):\n",
    "            if layer_name not in norms_per_layer:\n",
    "                norms_per_layer[layer_name] = []\n",
    "            norms_per_layer[layer_name].append(norm)\n",
    "    return norms_per_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LightningModule\n",
    "class ConvNet(pl.LightningModule):\n",
    "    def __init__(self, params: Dict[str,torch.nn]):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *params.get('conv_layers')\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            self.fc_layer = nn.Sequential(\n",
    "                *params.get('fc_layer')\n",
    "            )\n",
    "        except TypeError:\n",
    "            self.fc_layer = nn.Sequential(\n",
    "                params.get('fc_layer')\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    # Define the training step method\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    # Define the validation step method\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        self.log('val_loss', loss, prog_bar=True)  # Logging the validation loss\n",
    "    \n",
    "    # Define the test step method\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        self.log('test_loss', loss)  # Logging the test loss\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log('test_acc', acc, prog_bar=True)  # Logging the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create trainer object\n",
    "# trainer = pl.Trainer(accelerator='mps',max_epochs=1,devices=1)  # Set max_epochs and gpus according to your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir: str = os.path.join(os.getcwd(),'models')\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/adebayobraimah/Desktop/projects/fall2023project/models'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | conv_layers | Sequential | 387 K \n",
      "1 | fc_layer    | Sequential | 34.2 K\n",
      "-------------------------------------------\n",
      "422 K     Trainable params\n",
      "0         Non-trainable params\n",
      "422 K     Total params\n",
      "1.688     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model.1:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0048cdb637884ebc9bfc7fa13b298e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40309757eac14bcfa3cd9b534fb9cd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a30f51c90a4b48b22669a4e4ae14d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, train_loader, val_loader)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Save trained model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m save_pt_model(model\u001b[39m=\u001b[39;49mmodel,filename\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodels/\u001b[39;49m\u001b[39m{\u001b[39;49;00mparam\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m MARKDOWN \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m### \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m![](models/\u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m.png)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36msave_pt_model\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ext: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Save model (and model state)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mfilename\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mext\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     21\u001b[0m \u001b[39m# # pickle\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# with open(f\"{filename}{ext}\",'wb') as f:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#     pickle.dump(model,f)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m# Save image of model architecture\u001b[39;00m\n\u001b[1;32m     26\u001b[0m _save_and_viz_pl_model(model\u001b[39m=\u001b[39mmodel, filename\u001b[39m=\u001b[39mfilename)\n",
      "File \u001b[0;32m~/Desktop/projects/fall2023project/.env/lib/python3.11/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/fall2023project/.env/lib/python3.11/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/Desktop/projects/fall2023project/.env/lib/python3.11/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "MARKDOWN: str = \"\"\n",
    "\n",
    "for param in params.keys():\n",
    "    # Create trainer object\n",
    "    trainer = pl.Trainer(accelerator='mps',max_epochs=1,devices=1)  # Set max_epochs and gpus according to your environment\n",
    "    \n",
    "    # Print Model number to screen\n",
    "    print(f\"\\n{param}:\\n\")\n",
    "    \n",
    "    # Initialize the Lightning Trainer\n",
    "    model = ConvNet(params=params.get(param))\n",
    "\n",
    "    # Train the model using PyTorch Lightning\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Save trained model\n",
    "    save_pt_model(model=model,filename=f\"models/{param}\")\n",
    "\n",
    "    MARKDOWN += f\"### {param}: \\n\\n![](models/{param}.png)\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model architecture diagrams\n",
    "md(MARKDOWN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Models' Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict: Dict[str,Dict[str,Any]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm calcuation booleans\n",
    "weight: bool = True\n",
    "bias: bool = False\n",
    "\n",
    "for param in params.keys():\n",
    "    # Print Model number to screen\n",
    "    print(f\"\\n{param}:\\n\")\n",
    "\n",
    "    tmp_dict = {}\n",
    "\n",
    "    # Load model\n",
    "    model = load_pt_model(model=ConvNet(params=params.get(param)),filename=f\"models/{param}\")\n",
    "    # trainer = pl.Trainer(accelerator='mps',max_epochs=10,devices=1)  # Set max_epochs and gpus according to your environment\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    print(f\"Training accuracy:\")\n",
    "    train_results = trainer.test(model, dataloaders=train_loader)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    print(f\"Testing accuracy:\")\n",
    "    test_results = trainer.test(model, dataloaders=test_loader)\n",
    "\n",
    "    # Countable parameters\n",
    "    print(f\"Number of trainable parameters (weights): {count_trainable_parameters(model=model):,}\")\n",
    "    countable_parameters: str = f\"{count_trainable_parameters(model=model):,}\"\n",
    "\n",
    "    # Gradient Norms\n",
    "    grad_norms = get_gradient_norms(model=model, weight=weight, bias=bias)\n",
    "\n",
    "    # Layerwise Norms\n",
    "    layer_norms = get_layerwise_norms(model=model, weight=weight, bias=bias)\n",
    "\n",
    "    # Total parameter norm\n",
    "    total_norm = calculate_total_parameter_norm(model=model)\n",
    "\n",
    "    # Parameter norms per layer\n",
    "    norms_per_layer = calculate_parameter_norms_per_layer(model=model, weight=weight, bias=bias)\n",
    "\n",
    "    tmp_dict = {\n",
    "        \"train_acc\": f\"{train_results[0].get('test_acc'):.4f}\",\n",
    "        \"test_acc\": f\"{test_results[0].get('test_acc'):.4f}\",\n",
    "        \"parameters\": countable_parameters,\n",
    "        \"grad_norm\": grad_norms,\n",
    "        \"layer_norm\": layer_norms,\n",
    "        \"total_norm\": f\"{total_norm:.4f}\",\n",
    "        \"norms_per_layer\": norms_per_layer,\n",
    "    }\n",
    "\n",
    "    results_dict.update({param:tmp_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKDOWN = \"\"\"\n",
    "\n",
    "Table of metrics for each of the models defined above.\n",
    "\n",
    "| model \\ specifications | Train Accuracy | Test Accuracy | Number of Trainable Parameters | Gradient Norm | Layerwise Norm | Total Parameter Norm | Per Layer Parameter Norm |\n",
    "|------------------------|----------------|---------------|--------------------------------|---------------|----------------|----------------------|--------------------------|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in markdown table\n",
    "for name, metric in results_dict.items():\n",
    "    # print(f\"{name}: {metric}\")\n",
    "    MARKDOWN += f\"| **{name}** | {metric.get('train_acc')} | {metric.get('test_acc')} | {metric.get('parameters')} | {metric.get('grad_norm')} | {metric.get('layer_norm')} | {metric.get('total_norm')} | {metric.get('norms_per_layer')} |\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table of metrics for each model\n",
    "md(MARKDOWN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
